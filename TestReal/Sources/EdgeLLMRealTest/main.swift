import Foundation
import EdgeLLM

print("ğŸš€ EdgeLLM Real Inference Test")

func testRealInference() async {
    do {
        print("ğŸ“ Testing EdgeLLM with local Qwen3-0.6B model...")
        
        // ãƒ†ã‚¹ãƒˆ1: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®ç¢ºèª
        let modelPath = EdgeLLM.localModelPaths[.qwen05b]
        print("ğŸ” Model path: \(modelPath ?? "Not found")")
        
        // ãƒ†ã‚¹ãƒˆ2: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if let path = modelPath {
            let fileManager = FileManager.default
            let exists = fileManager.fileExists(atPath: path)
            print("ğŸ“‚ Model exists: \(exists ? "âœ… Yes" : "âŒ No")")
            
            if exists {
                print("ğŸ“Š Model files:")
                let configPath = "\(path)/mlc-chat-config.json"
                let tokenizerPath = "\(path)/tokenizer.json"
                print("  - Config: \(fileManager.fileExists(atPath: configPath) ? "âœ…" : "âŒ")")
                print("  - Tokenizer: \(fileManager.fileExists(atPath: tokenizerPath) ? "âœ…" : "âŒ")")
            }
        }
        
        // ãƒ†ã‚¹ãƒˆ3: å®Ÿéš›ã®æ¨è«–ãƒ†ã‚¹ãƒˆ
        print("\nğŸ¤– Attempting real inference...")
        let testPrompt = "Hello, how are you?"
        
        // ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã®ãƒãƒ£ãƒƒãƒˆã‚’è©¦è¡Œ
        let response = try await EdgeLLM.chatWithLocalModel(
            testPrompt,
            model: .qwen05b
        )
        
        print("âœ… Success! Response:")
        print("ğŸ“¤ Input: \(testPrompt)")
        print("ğŸ“¥ Output: \(response)")
        
    } catch {
        print("âŒ Error during inference test: \(error)")
        
        if let edgeError = error as? EdgeLLMError {
            print("   EdgeLLM Error: \(edgeError.localizedDescription)")
        }
        
        print("\nğŸ’¡ This is expected if:")
        print("   - MLC-LLM libraries are not properly linked")
        print("   - Model files are not in the correct format")
        print("   - GPU/Metal is not available")
    }
}

// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
Task {
    await testRealInference()
    print("\nğŸ Test completed.")
    exit(0)
}

RunLoop.main.run()